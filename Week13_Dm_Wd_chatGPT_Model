{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrHN9irRZXyB2MkdlZeDcw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EN8y66YU-DRk","executionInfo":{"status":"ok","timestamp":1713325269840,"user_tz":300,"elapsed":2151,"user":{"displayName":"Pavan Kumar Battula","userId":"16738401156277944723"}},"outputId":"822b16e3-46f3-4efb-eff6-59286e44674e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Valparaiso University is ...\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding, Concatenate\n","from tensorflow.keras.models import Model\n","\n","class GPT(Model):\n","    def __init__(self, vocab_size, max_sequence_length, d_model, num_heads, num_layers, dropout_rate=0.1):\n","        super(GPT, self).__init__()\n","\n","        # Define embedding layer\n","        self.embedding_layer = Embedding(vocab_size, d_model)\n","\n","        # Define positional encoding layer\n","        self.positional_encoding = self.get_positional_encoding(max_sequence_length, d_model)\n","\n","        # Define transformer layers\n","        self.transformer_layers = [self.create_transformer_layer(d_model, num_heads, dropout_rate) for _ in range(num_layers)]\n","\n","        # Define output layer\n","        self.output_layer = Dense(vocab_size)\n","\n","    def get_positional_encoding(self, max_sequence_length, d_model):\n","        # Calculate positional encodings\n","        positional_encoding = []\n","        for pos in range(max_sequence_length):\n","            pos_encoding = [pos / tf.pow(tf.constant(10000, dtype=tf.float32), 2 * (i // 2) / tf.cast(d_model, tf.float32)) for i in range(d_model)]\n","            if pos % 2 == 0:\n","                positional_encoding.append(tf.math.sin(pos_encoding))\n","            else:\n","                positional_encoding.append(tf.math.cos(pos_encoding))\n","        positional_encoding = tf.stack(positional_encoding)\n","        return tf.expand_dims(positional_encoding, axis=0)\n","\n","    def create_transformer_layer(self, d_model, num_heads, dropout_rate):\n","        # Create transformer layer\n","        return MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads, dropout=dropout_rate)\n","\n","    def call(self, inputs, training=False):\n","        # Define forward pass of the model\n","        input_sequence = inputs\n","\n","        # Embed input sequence and add positional encoding\n","        embedded_input = self.embedding_layer(input_sequence) + self.positional_encoding[:, :tf.shape(input_sequence)[1], :]\n","\n","        # Apply transformer layers sequentially\n","        transformer_output = embedded_input\n","        for layer in self.transformer_layers:\n","            transformer_output = layer(query=transformer_output, value=transformer_output, attention_mask=None, training=training)\n","\n","        # Apply output layer\n","        output_logits = self.output_layer(transformer_output)\n","\n","        return output_logits\n","\n","    def get_output_probabilities(self, inputs):\n","        # Call the model to get output logits\n","        output_logits = self(inputs)\n","\n","        # Apply softmax to obtain output probabilities\n","        output_probs = tf.nn.softmax(output_logits, axis=-1)\n","\n","        return output_probs\n","\n","# Instantiate the GPT model with desired parameters\n","vocab_size = 10000  # Example vocabulary size\n","max_sequence_length = 50  # Example maximum sequence length\n","d_model = 128  # Example model dimensionality\n","num_heads = 4  # Example number of attention heads\n","num_layers = 2  # Example number of transformer layers\n","dropout_rate = 0.1  # Example dropout rate\n","gpt_model = GPT(vocab_size, max_sequence_length, d_model, num_heads, num_layers, dropout_rate)\n","\n","# Prepare input data (replace this with your actual input data)\n","input_sequence = tf.constant([[1, 2, 3, 4, 0, 0]])  # Example input sequence\n","\n","# Get output probabilities\n","output_probs = gpt_model.get_output_probabilities(input_sequence)\n","\n","# Output probabilities shape: (batch_size, sequence_length, vocab_size)\n","print(\"Valparaiso University is ...\")\n"]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load pre-trained GPT-2 model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Define input text\n","input_text = \"Valparaiso University is\"\n","\n","# Tokenize input text\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","\n","# Generate text\n","output = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=1.0)\n","\n","# Decode and print generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(\"Generated Text:\", generated_text)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFFWQFGULHHW","executionInfo":{"status":"ok","timestamp":1713325943051,"user_tz":300,"elapsed":5840,"user":{"displayName":"Pavan Kumar Battula","userId":"16738401156277944723"}},"outputId":"19150b03-b183-416f-b8c7-10921e35b4dc"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Text: Valparaiso University is a research and teaching institution in the University of California, Berkeley. The School of Medicine is a research and teaching institution in the University of California, Berkeley. The School of Medicine is a research and teaching institution in the University\n"]}]}]}